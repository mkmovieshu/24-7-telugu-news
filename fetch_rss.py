import feedparser
import hashlib
import logging
import os
from pymongo import errors as mongo_errors # MongoDB ‡∞≤‡±ã‡∞™‡∞Ç ‡∞ï‡±ã‡∞∏‡∞Ç
from datetime import datetime
from summarize import summarize_news

# Import DB connection and collection from db.py
from db import news_collection as news_col, client, DB_NAME

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
log = logging.getLogger("fetch_rss")

try:
    # ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞®‡±Å ‡∞§‡∞®‡∞ø‡∞ñ‡±Ä ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞í‡∞ï ‡∞ö‡∞ø‡∞®‡±ç‡∞® ‡∞Ü‡∞™‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
    client.admin.command('ping') 
    log.info("MongoDB ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞Ç‡∞§‡∞Ç: DB='%s'", DB_NAME)

except mongo_errors.ConnectionFailure as e:
    log.error("MongoDB ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞≤‡±ã‡∞™‡∞Ç: %s", e)
    exit(1)
except Exception as e:
    log.error("‡∞∏‡∞æ‡∞ß‡∞æ‡∞∞‡∞£ ‡∞ï‡∞®‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞≤‡±ã‡∞™‡∞Ç: %s", e)
    exit(1)


# ‚úÖ ‡∞Æ‡±Ü‡∞∞‡±Å‡∞ó‡±à‡∞® RSS ‡∞´‡±Ä‡∞°‡±ç URL‡∞≤‡±Å
RSS_FEEDS = [
    "https://telugu.news18.com/commonfeeds/v1/tel/rss/andhra-pradesh.xml",
    "https://telugu.news18.com/commonfeeds/v1/tel/rss/international.xml",
    "https://telugu.news18.com/commonfeeds/v1/tel/rss/national.xml",
    "https://telugu.news18.com/commonfeeds/v1/tel/rss/telangana.xml",
]

# ... make_hash ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞Æ‡±Ä ‡∞ï‡±ã‡∞°‡±ç ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ...
def make_hash(title: str, source: str) -> str:
    base = f"{source}|{title.strip().lower()}"
    return hashlib.sha256(base.encode("utf-8")).hexdigest()

def fetch():
    for feed_url in RSS_FEEDS:
        log.info("‡∞´‡±Ü‡∞ö‡∞ø‡∞Ç‡∞ó‡±ç: %s", feed_url)
        feed = feedparser.parse(feed_url)

        if feed.status not in (200, 301, 302) and feed.status != 304:
            log.warning("‡∞´‡±Ä‡∞°‡±ç ‡∞≤‡±ã‡∞°‡±ç ‡∞ï‡∞æ‡∞≤‡±á‡∞¶‡±Å (Status: %s): %s", feed.status, feed_url)
            continue
            
        source = feed.feed.get("title", "unknown")

        for entry in feed.entries:
            try:
                title = entry.get("title", "").strip()
                link = entry.get("link", "").strip()

                if not title or not link:
                    continue

                h = make_hash(title, source)

                # üîí HARD STOP: DUPLICATE
                if news_col.find_one({"hash": h}):
                    log.info("SKIP (cached): %s", title)
                    continue

                content = entry.get("summary", "") or entry.get("description", "")

                # üß† AI only if really needed
                # üö® ‡∞ó‡∞Æ‡∞®‡∞ø‡∞ï: summarize_news ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞Æ‡±Ä summarize.py ‡∞´‡±à‡∞≤‡±ç‚Äå‡∞≤‡±ã ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø
                summary, ai_used = summarize_news(title, content)

                doc = {
                    "title": title,
                    "summary": summary,
                    "source": source,
                    "link": link,
                    "hash": h,
                    "ai_used": ai_used,
                    "created_at": datetime.utcnow()
                }

                news_col.insert_one(doc)
                log.info("INSERTED: %s | AI=%s", title[:50], ai_used)

            except Exception as e:
                log.error("‡∞®‡±ç‡∞Ø‡±Ç‡∞∏‡±ç ‡∞é‡∞Ç‡∞ü‡±ç‡∞∞‡±Ä ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç‚Äå‡∞≤‡±ã ‡∞≤‡±ã‡∞™‡∞Ç: %s", e)


if __name__ == "__main__":
    fetch()
